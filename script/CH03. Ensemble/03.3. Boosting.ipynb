{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH03.3. **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 여러 약한 모델(학습기)을 순차적으로 학습시켜 강한 모델을 만드는 앙상블 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 순차적 학습 : 이전 단계의 결과를 이용하여 다음 학습기를 학습함\n",
    "##### $ \\hspace{0.15cm} $ ② 오류 보완 : 이전 단계의 약한 모델의 오류를 보완하여 다음 모델이 이를 집중적으로 학습할 수 있도록 처리함\n",
    "##### $ \\hspace{0.15cm} $ ③ 특성 중요도(Feature Importance) 추출 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "##### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려\n",
    "##### $ \\hspace{0.15cm} $ ② 데이터 노이즈(noise)에 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(5) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① AdaBoost\n",
    "##### $ \\hspace{0.15cm} $ ② Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **AdaBoost(Adaptive Boost)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 이전 학습기에서 잘못 분류된 데이터(샘플)의 추출 가중치를 증가함으로써 오류에 대한 민감성을 줄이는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 단순한 약한 학습기 사용 : 아주 간단한 모델(스텀프;$  1 $ 회 분기처리한 트리)을 사용하여 복잡성을 관리\n",
    "##### $ \\hspace{0.15cm} $ ② 가중치 조정 : 각 데이터 포인트에 가중치를 부여하여, 잘못 분류된 데이터를 더 많이 샘플링함 \n",
    "##### $ \\hspace{0.15cm} $ ③ 가중 합산 : 최종 예측은 각 약한 학습기의 예측에 가중치를 부여하여 합산한 결과로 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "##### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려\n",
    "##### $ \\hspace{0.15cm} $ ② 데이터 노이즈(noise;$ \\epsilon{} $)에 민감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 손실 함수의 그레이디언트를 사용하여 새로운 약한 모델을 추가하며 전체 모델의 성능을 점진적으로 향상시키는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 단순한 약한 학습기 사용 : 간단한 모델을 사용하여 복잡성을 관리\n",
    "##### $ \\hspace{0.15cm} $ ② 손실 함수 최적화 : 특정 손실 함수를 최소화하는 방향으로 모델을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 높은 정확도 : 순차적으로 학습하면서 이전 단계의 오류를 보완\n",
    "##### $ \\hspace{0.15cm} $ ② 유연성 : 많은 종류의 약한 학습기를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 과적합(overfitting) 우려 : 약한 모델이 많아질수록 노이즈까지 학습할 가능성이 높음\n",
    "##### $ \\hspace{0.15cm} $ ② 하이퍼파라미터 조정 복잡 : 다양한 하이퍼파라미터를 조정해야 하므로 복잡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) 과적합 방지를 위한 정규화(regularization)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ sub-sampling : 약한 모델별로 원본 데이터셋 무작위로 (복원, 비복원 상관없이) 추출하여 학습\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ shirinkage : 약한 모델의 취합할 때 후속 모델일수록 가중치를 낮게 잡아 예측값 산출\n",
    "##### $ \\hspace{0.45cm} \\text{ex. } \\; \\hat{y} = (0.9)^{0}f_{1} + (0.9)^{1}f_{2} + (0.9)^{2}f_{3} + \\cdots{} + (0.9)^{n-1}f_{n} $ \n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ early-stopping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
