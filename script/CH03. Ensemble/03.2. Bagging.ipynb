{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH03.2. **Bagging**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Bagging(Bootstrap Aggregating)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 원본 학습 데이터에서 여러 개의 **복원 추출** 샘플을 만들어 각 샘플마다 독립적으로 개별 모델을 학습한 뒤, \n",
    "#### $ \\hspace{1.05cm} $ 개별 모델의 예측 결과를 결합하여 최종 예측하는 앙상블 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 저편향-고분산 모델에 적합하게 작동 (ex. DT, nonlinear-SVM, DNN)\n",
    "##### $ \\hspace{0.15cm} $ ② OOB(Out of Bag) 데이터로 인해 검증용 추가 데이터 불필요\n",
    "##### $ \\hspace{0.15cm} $ ③ 병렬 연산(pararell computing) 처리 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) Out of Bag 데이터** : 어떤 샘플이 복원 추출할 때, 추출 과정에서 선택되지 않은 데이터(전체 중 $ \\, 36.8\\% $ 의 데이터)\n",
    "#### $ \\Rightarrow{} \\displaystyle\\lim_{N\\rightarrow{}\\infty{}}p = \\displaystyle\\lim_{N\\rightarrow{}\\infty{}}(1 - \\frac{1}{N})^{N} = e^{-1} \\approx{} 0.368 $\n",
    "#### $ \\hspace{0.15cm} \\text{where } \\, p = (1 - \\frac{1}{N})^{N} : $ 원본 데이터에서 $ \\, N $ 번 추출할 때 그 특정 데이터가 선택되지 않을 확률\n",
    "#### $ \\hspace{0.15cm} \\text{and } \\, N : $ 원본 데이터의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 분산(Variance) 감소\n",
    "##### $ \\hspace{0.15cm} $ ② 과적합 감소(방지) : 복원 추출을 통해 학습 데이터의 noise 정도(분포)를 바꾸어 긍정적인 데이터 왜곡 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 해석력(Interpretability) 저하\n",
    "##### $ \\hspace{0.15cm} $ ② 편향(Bias)을 크게 낮추지는 못함\n",
    "##### $ \\hspace{0.15cm} $ ③ 데이터 중복으로 인한 학습 비용 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 기존 배깅 나무(bagging tree)에서 특성을 무작위로 선택 및 훈련하여 다양성을 확보한 앙상블 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 트리 생성 과정에서 노드 분할 시 무작위로 선택된 특성의 부분 집합(feature subset)만 고려\n",
    "##### $ \\hspace{0.15cm} $ ② 변수 중요도 평가 가능\n",
    "##### $ \\hspace{0.15cm} $ ③ 병렬 연산 처리 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 높은 정확도 기대 : (배깅 트리에 비해) 전체적인 예측 성능 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 과적합 감소(방지) 기대 : 기존 배깅 트리는 데이터셋이 유사할 가능성이 존재해 다양성(diversity)가 부족할 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 해석력(Interpretability) 저하\n",
    "##### $ \\hspace{0.15cm} $ ② 중요도가 낮은 특징은 잘 안 쓰이기도 함\n",
    "##### $ \\hspace{0.15cm} $ ③ 학습 비용 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`) Generalization Error** :\n",
    "#### $ \\hspace{0.15cm} $ **[LATEX]**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
