{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH03.1. **Ensemble Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 여러 개의 모델(Weak Learners;Base Learners)들을 학습시킨 후, 이들의 예측을 종합하여 최종 예측값을 내는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`WHY?`)** 어떠한 개별 모델(weak learner)들도 모든 데이터셋에 대해 우월할 수 없기 때문에, 이를 조합함으로써 우수한 성능을 기대할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "#### $ \\hspace{0.15cm} $ ① Bias-Variance Trade-off 개선\n",
    "#### $ \\hspace{0.15cm} $ ② 안정성(robustness) 향상 : 단일 모델이 특정 데이터 분포에 치우쳐 있을 때, 여러 모델의 평균화를 통해 극단적인 예측이나 과적합 위험을 낮춤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 성능 향상 : 일반적으로 단일 모델보다 더 우수한 성능을 기대\n",
    "#### $ \\hspace{0.15cm} $ ② 과적합 감소(방지)\n",
    "#### $ \\hspace{0.15cm} $ ③ 유연성 : 서로 다른 모델을 결합하여 메타 모델을 구성할 수 있어 문제에 따라 다양한 조합이 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "#### $ \\hspace{0.15cm} $ ① 해석력(Interpretability) 저하\n",
    "#### $ \\hspace{0.15cm} $ ② 계산 비용 증가\n",
    "#### $ \\hspace{0.15cm} $ ③ 데이터 요구량 증가 : 여러 모델을 훈련시키기 위해서는 일정 수준 이상의 데이터가 필요(적다면 과적합 가능성 높음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Bias-Variance Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 회귀 문제에서 모델이 갖는 평균제곱오차를 편향과 분산, 줄일 수 없는 잡음(Irreducible Error)로 분해하여 표현하는 방법\n",
    "#### $ \\Rightarrow{} \\text{Err}(x_{0}) = \\text{MSE}(x_{0}) = \\text{bias}(\\hat{F}(x_{0}))+\\text{var}(\\hat{F}(x_{0})) + \\sigma{}^{2} $\n",
    "#### $ \\hspace{0.15cm} \\text{where } \\, \\epsilon{} \\sim{} \\text{i.i.d.} \\;\\; \\text{ and } \\, \\epsilon{} \\sim{} N(0, \\sigma{}^{2}) $\n",
    "#### $ \\hspace{0.15cm} \\text{and } \\, \\text{bias}(\\hat{F}(x_{0})) : $ 학습했을 때 평균 예측값과 진짜 참값(Truth) 사이의 차이\n",
    "#### $ \\hspace{0.15cm} \\text{and } \\, \\text{var}(\\hat{F}(x_{0})) : $ 개별 모델들의 예측값이 그들의 평균에서 퍼져있는 정도($ \\approx{} $ spread)\n",
    "#### $ \\hspace{0.15cm} \\text{and } \\, \\sigma{}^{2} : $ 데이터 자체에 내재된 노이즈, 혹은 통제할 수 없는 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`WHY?`)**\n",
    "#### $ \\Rightarrow{} \\text{Err}(x_{0}) = \\text{MSE}(x_{0}) = E[(y_{x_{0}}-\\hat{F}(x_{0}))^{2}] $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})+\\epsilon{}-\\hat{F}(x_{0}))^{2}] \\;\\; $ ($ \\because{} \\, y = F^{*}(\\textbf{x}) + \\epsilon{} $ )\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\hat{F}(x_{0}))^{2}+(\\epsilon{})^{2}-2(F^{*}(x_{0})-\\hat{F}(x_{0}))\\epsilon{}] $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\hat{F}(x_{0}))^{2}]+E[(\\epsilon{})^{2}]-E[2(F^{*}(x_{0})-\\hat{F}(x_{0}))\\epsilon{}] $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\hat{F}(x_{0}))^{2}]+E[(\\epsilon{})^{2}]-2E[(F^{*}(x_{0})-\\hat{F}(x_{0}))]E[\\epsilon{}] $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\hat{F}(x_{0}))^{2}]+E[(\\epsilon{})^{2}] \\;\\; $ ($ \\because{} \\, E(\\epsilon{}) = 0 $ )\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\hat{F}(x_{0}))^{2}]+\\sigma{}^{2} $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\bar{F}(x_{0})+\\bar{F}(x_{0})-\\hat{F}(x_{0}))^{2}]+\\sigma{}^{2} $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\bar{F}(x_{0}))^{2}+(\\bar{F}(x_{0})-\\hat{F}(x_{0}))^{2}-2(F^{*}(x_{0})-\\bar{F}(x_{0}))(\\bar{F}(x_{0})-\\hat{F}(x_{0}))]+\\sigma{}^{2} $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\bar{F}(x_{0}))^{2}]+E[(\\bar{F}(x_{0})-\\hat{F}(x_{0}))^{2}]-E[2(F^{*}(x_{0})-\\bar{F}(x_{0}))(\\bar{F}(x_{0})-\\hat{F}(x_{0}))]+\\sigma{}^{2} $\n",
    "#### $ \\hspace{1.675cm} = E[(F^{*}(x_{0})-\\bar{F}(x_{0}))^{2}]+E[(\\bar{F}(x_{0})-\\hat{F}(x_{0}))^{2}]+\\sigma{}^{2} \\;\\; $ ($ \\because{} \\, \\bar{F}(x_{0}) = E[\\hat{F}(x_{0})] $ )\n",
    "#### $ \\hspace{1.675cm} = (F^{*}(x_{0})-\\bar{F}(x_{0}))^{2}+E[(\\bar{F}(x_{0})-\\hat{F}(x_{0}))^{2}]+\\sigma{}^{2} \\;\\; $ ($ \\because{} \\, F^{*}(x_{0}) \\, \\text{ is constant.} $)\n",
    "#### $ \\hspace{1.675cm} = \\text{bias}(\\hat{F}(x_{0}))+\\text{var}(\\hat{F}(x_{0})) + \\sigma{}^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 편향(Bias)과 분산(Variance)의 관계**(모델 복잡도에 대한 관점) : \n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델 복잡도가 낮을수록 해당 데이터에 대한 편향(예측 성능)이 낮고, 데이터가 바뀌어도 크게 달라지지 않음(저분산)\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델 복잡도가 올라갈수록 해당 데이터에 대한 편향이 올라가나, 데이터가 바뀌어도 분산이 커짐(고분산)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`)** 머신러닝에서는 이 양쪽을 적절히 균형 잡아, 최소의 오차를 유도하도록 모델과 하이퍼파라미터를 조정하는 것이 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 보완 방법** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배깅(Bagging) : 분산 감소 기대\n",
    "##### $ \\hspace{0.15cm} $ ② 부스팅(Boosting) : 편향 감소 기대\n",
    "##### $ \\hspace{0.15cm} $ ③ 스태킹(stacking) : 하이브리드(분산 및 편향 감소 기대)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`)** 얼마나 서로 다른 모델(다양성;Diversity)을 확보하며, 어떻게 결합할 것이에 대한 결정이 앙상블의 핵심임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Effect of Ensemble**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1)** 여러 모델의 예측을 평균해서 사용하면, 각각의 모델이 내는 잡음(noise)이 상쇄되어 전체 오류가 줄어들 수 있음 \n",
    "#### $ \\Rightarrow{} E_{\\text{ensemble}} = \\frac{1}{M} E_{\\text{avg}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`WHY?`)**\n",
    "#### **[LATEX]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Result Aggregating Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 분류에서의 결과 취합** : \n",
    "##### $ \\hspace{0.15cm} $ ① 다수결 투표(Majority;Hard Voting) : **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ② 확률적 투표(Probabilistic Voting) : **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ③ 가중치 투표(Weighted Voting) : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 회귀에서의 결과 취합** : **[CONTENTS]**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
